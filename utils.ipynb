{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "def plot_learning_curve(clf, X, y, title=\"Insert Title\"):\n",
    "    n_samples, n_features = X.shape\n",
    "    n = len(y)\n",
    "    train_mean = []; train_std = []\n",
    "    cv_mean = []; cv_std = []\n",
    "    fit_mean = []; fit_std = []\n",
    "    pred_mean = []; pred_std = []\n",
    "    train_sizes = (np.linspace(.05, 1.0, 20) * n).astype('int')\n",
    "\n",
    "\n",
    "    for i in train_sizes:\n",
    "        idx = np.random.randint(n_samples, size=i)\n",
    "        \n",
    "        # Ensure that the indices are within the valid range\n",
    "        idx = np.clip(idx, 0, n_samples - 1)\n",
    "\n",
    "\n",
    "        X_subset = X.iloc[idx, :]  # Use iloc for DataFrame indexing\n",
    "        y_subset = y.iloc[idx]\n",
    "        scores = cross_validate(clf, X_subset, y_subset, cv=10, scoring='f1', n_jobs=-1, return_train_score=True)\n",
    "\n",
    "        train_mean.append(np.mean(scores['train_score']))\n",
    "        train_std.append(np.std(scores['train_score']))\n",
    "        cv_mean.append(np.mean(scores['test_score']))\n",
    "        cv_std.append(np.std(scores['test_score']))\n",
    "        fit_mean.append(np.mean(scores['fit_time']))\n",
    "        fit_std.append(np.std(scores['fit_time']))\n",
    "        pred_mean.append(np.mean(scores['score_time']))\n",
    "        pred_std.append(np.std(scores['score_time']))\n",
    "\n",
    "    train_mean = np.array(train_mean)\n",
    "    train_std = np.array(train_std)\n",
    "    cv_mean = np.array(cv_mean)\n",
    "    cv_std = np.array(cv_std)\n",
    "    fit_mean = np.array(fit_mean)\n",
    "    fit_std = np.array(fit_std)\n",
    "    pred_mean = np.array(pred_mean)\n",
    "    pred_std = np.array(pred_std)\n",
    "\n",
    "    plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title)\n",
    "    plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title)\n",
    "\n",
    "    return train_sizes, train_mean, fit_mean, pred_mean\n",
    "\n",
    "\n",
    "\n",
    "def plot_LC(train_sizes, train_mean, train_std, cv_mean, cv_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Learning Curve: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Model F1 Score\")\n",
    "    plt.fill_between(train_sizes, train_mean - 2*train_std, train_mean + 2*train_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, cv_mean - 2*cv_std, cv_mean + 2*cv_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color=\"b\", label=\"Training Score\")\n",
    "    plt.plot(train_sizes, cv_mean, 'o-', color=\"r\", label=\"Cross-Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_times(train_sizes, fit_mean, fit_std, pred_mean, pred_std, title):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Modeling Time: \"+ title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Training Time (s)\")\n",
    "    plt.fill_between(train_sizes, fit_mean - 2*fit_std, fit_mean + 2*fit_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(train_sizes, pred_mean - 2*pred_std, pred_mean + 2*pred_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(train_sizes, fit_mean, 'o-', color=\"b\", label=\"Training Time (s)\")\n",
    "    plt.plot(train_sizes, pred_std, 'o-', color=\"r\", label=\"Prediction Time (s)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(2), range(2)):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    \n",
    "def final_classifier_evaluation(clf,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = timeit.default_timer()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    start_time = timeit.default_timer()    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = timeit.default_timer()\n",
    "    pred_time = end_time - start_time\n",
    "    \n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test,y_pred)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    recall = recall_score(y_test,y_pred)\n",
    "    cm = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "    print(\"Model Evaluation Metrics - Test Dataset\")\n",
    "    print(\"*****************************************************\")\n",
    "    print(\"Model Training Time (s):   \"+\"{:.5f}\".format(training_time))\n",
    "    print(\"Model Prediction Time (s): \"+\"{:.5f}\\n\".format(pred_time))\n",
    "    print(\"F1 Score:  \"+\"{:.2f}\".format(f1))\n",
    "    print(\"Accuracy:  \"+\"{:.2f}\".format(accuracy)+\"     AUC:       \"+\"{:.2f}\".format(auc))\n",
    "    print(\"Precision: \"+\"{:.2f}\".format(precision)+\"     Recall:    \"+\"{:.2f}\".format(recall))\n",
    "    print(\"*****************************************************\")\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=[\"0\",\"1\"], title='Confusion Matrix')\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_complexity_curve_decision_tree(X_train, y_train, X_test, y_test, title):\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    max_depth = list(range(1,31))\n",
    "    for i in max_depth:         \n",
    "            clf = DecisionTreeClassifier(max_depth=i, random_state=100, min_samples_leaf=1, criterion='entropy')\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(max_depth, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(max_depth, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('Max Tree Depth')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')    \n",
    "    plt.show()\n",
    "     \n",
    "    \n",
    "def cross_validation_decision_tree(start_leaf_n, end_leaf_n, X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #20 values of min_samples leaf from 0.5% sample to 5% of the training data\n",
    "    #20 values of max_depth from 1, 20\n",
    "    param_grid = {'min_samples_leaf':np.linspace(start_leaf_n,end_leaf_n,20).round().astype('int'), 'max_depth':np.arange(1,20)}\n",
    "\n",
    "    tree = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid=param_grid, cv=10)\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(\"Per Hyperparameter tuning, best parameters are:\")\n",
    "    print(tree.best_params_)\n",
    "    return tree.best_params_['max_depth'], tree.best_params_['min_samples_leaf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_complexity_curve_neural_network(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    hlist = np.linspace(1,150,30).astype('int')\n",
    "    for i in hlist:         \n",
    "            clf = MLPClassifier(hidden_layer_sizes=(i,), solver='adam', activation='logistic', \n",
    "                                learning_rate_init=0.05, max_iter=400, random_state=100)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(hlist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(hlist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Hidden Units')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def cross_validation_neural_network(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    h_units = [5, 10, 20, 30, 40, 50, 75, 100]\n",
    "    learning_rates = [0.01, 0.05, .1]\n",
    "    param_grid = {'hidden_layer_sizes': h_units, 'learning_rate_init': learning_rates}\n",
    "\n",
    "    net = GridSearchCV(estimator = MLPClassifier(solver='adam',activation='logistic',random_state=100),\n",
    "                       param_grid=param_grid, cv=10)\n",
    "    net.fit(X_train, y_train)\n",
    "    print(f\"Cross-Validation Hyperparameters: {net.best_params_}\")\n",
    "    return net.best_params_['hidden_layer_sizes'], net.best_params_['learning_rate_init']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_complexity_curve_knn(X_train, y_train, X_test, y_test, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    klist = np.linspace(1,250,25).astype('int')\n",
    "    for i in klist:\n",
    "        clf = KNN(n_neighbors=i,n_jobs=-1)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        f1_test.append(f1_score(y_test, y_pred_test))\n",
    "        f1_train.append(f1_score(y_train, y_pred_train))\n",
    "        \n",
    "    plt.plot(klist, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(klist, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Neighbors')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')    \n",
    "    plt.show()\n",
    "    \n",
    "def cross_validation_knn(X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #number of hidden units\n",
    "    #learning_rate\n",
    "    n_neighbors = [5, 10, 15, 20, 25, 50, 100, 150, 200]\n",
    "    weights = ['uniform', 'distance']\n",
    "    metric = ['manhattan', 'euclidean']\n",
    "    param_grid = {\n",
    "        \"n_neighbors\": n_neighbors,\n",
    "        \"weights\": weights,\n",
    "\n",
    "        \"metric\": metric,\n",
    "    }\n",
    "\n",
    "    cls = GridSearchCV(estimator = KNN(), param_grid=param_grid, cv=10)\n",
    "    cls.fit(X_train, y_train)\n",
    "    print(f\"Cross-Validation Hyperparameters: {cls.best_params_}\")\n",
    "    return cls.best_params_['n_neighbors'], cls.best_params_['weights'], cls.best_params_['metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_complexity_curve_boosted_decision_tree(X_train, y_train, X_test, y_test, max_depth, min_samples_leaf, title):\n",
    "    \n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    n_estimators = np.linspace(1,250,40).astype('int')\n",
    "    for i in n_estimators:         \n",
    "            clf = GradientBoostingClassifier(n_estimators=i, max_depth=int(max_depth/2), \n",
    "                                             min_samples_leaf=int(min_samples_leaf/2), random_state=100,)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred_test = clf.predict(X_test)\n",
    "            y_pred_train = clf.predict(X_train)\n",
    "            f1_test.append(f1_score(y_test, y_pred_test))\n",
    "            f1_train.append(f1_score(y_train, y_pred_train))\n",
    "      \n",
    "    plt.plot(n_estimators, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(n_estimators, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('No. Estimators')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')    \n",
    "    plt.show()\n",
    "\n",
    "def cross_validation_boosted_decision_tree(start_leaf_n, end_leaf_n, X_train, y_train):\n",
    "    #parameters to search:\n",
    "    #n_estimators, learning_rate, max_depth, min_samples_leaf\n",
    "    param_grid = {'min_samples_leaf': np.linspace(start_leaf_n,end_leaf_n,3).round().astype('int'),\n",
    "                  'max_depth': np.arange(1,4),\n",
    "                  'n_estimators': np.linspace(10,100,3).round().astype('int'),\n",
    "                  'learning_rate': np.linspace(.001,.1,3)}\n",
    "\n",
    "    boost = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid=param_grid, cv=10)\n",
    "    boost.fit(X_train, y_train)\n",
    "    print(f\"Cross-Validation Hyperparameters: {boost.best_params_}\")\n",
    "    return boost.best_params_['max_depth'], boost.best_params_['min_samples_leaf'], boost.best_params_['n_estimators'], boost.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_complexity_curve_svm(X_train, y_train, X_test, y_test, title):\n",
    "\n",
    "    f1_test = []\n",
    "    f1_train = []\n",
    "    kernel_func = ['linear','poly','rbf','sigmoid']\n",
    "    for i in kernel_func:         \n",
    "            if i == 'poly':\n",
    "                for j in [2,3,4,5,6,7,8]:\n",
    "                    clf = SVC(kernel=i, degree=j,random_state=100)\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred_test = clf.predict(X_test)\n",
    "                    y_pred_train = clf.predict(X_train)\n",
    "                    f1_test.append(f1_score(y_test, y_pred_test))\n",
    "                    f1_train.append(f1_score(y_train, y_pred_train))\n",
    "            else:    \n",
    "                clf = SVC(kernel=i, random_state=100)\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred_test = clf.predict(X_test)\n",
    "                y_pred_train = clf.predict(X_train)\n",
    "                f1_test.append(f1_score(y_test, y_pred_test))\n",
    "                f1_train.append(f1_score(y_train, y_pred_train))\n",
    "                \n",
    "    xvals = ['linear','poly2','poly3','poly4','poly5','poly6','poly7','poly8','rbf','sigmoid']\n",
    "    plt.plot(xvals, f1_test, 'o-', color='r', label='Test F1 Score')\n",
    "    plt.plot(xvals, f1_train, 'o-', color = 'b', label='Train F1 Score')\n",
    "    plt.ylabel('Model F1 Score')\n",
    "    plt.xlabel('Kernel Function')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.annotate('fjb7@gatech.edu', xy=(0.3, 0.3), xycoords='axes fraction', rotation=45, alpha=0.5, fontsize=25, color='gray')\n",
    "    plt.show()\n",
    "    \n",
    "def cross_validation_svm(X_train, y_train):\n",
    "    c_values = [1e-4, 1e-3, 1e-2, 1e01, 1]\n",
    "    gammas = [1,10,100]\n",
    "    param_grid = {'C': c_values, 'gamma': gammas}\n",
    "        \n",
    "\n",
    "    clf = GridSearchCV(estimator = SVC(random_state=100), param_grid=param_grid, cv=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Cross-Validation Hyperparameters: {clf.best_params_}\")\n",
    "    return clf.best_params_['C'], clf.best_params_['gamma']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7641",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
